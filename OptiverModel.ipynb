{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b2e301-b6aa-4e7e-b8d4-3b355fdf18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  # Garbage collection for memory management\n",
    "import os  # Operating system-related functions\n",
    "import time  # Time-related functions\n",
    "import warnings  # Handling warnings\n",
    "from itertools import combinations  # For creating combinations of elements\n",
    "from warnings import simplefilter  # Simplifying warning handling   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# üì¶ Importing machine learning libraries\n",
    "import joblib  # For saving and loading models\n",
    "import lightgbm as lgb  # LightGBM gradient boosting framework\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n",
    "\n",
    "# ü§ê Disable warnings to keep the code clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# üìä Define flags and variables\n",
    "is_offline = False  # Flag for online/offline mode\n",
    "is_train = True  # Flag for training mode\n",
    "is_infer = True  # Flag for inference mode\n",
    "max_lookback = np.nan  # Maximum lookback (not specified)\n",
    "split_day = 435  # Split day for time series data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0671b3f8-fc20-4cc5-8080-8c04ef6e81f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237892, 17)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/shi/Documents/Trading/Kaggle/train.csv')\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape\n",
    "\n",
    "# test data starts at day 478 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697c9903-7f34-431d-828f-2321939b5ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>far_price</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>ask_price</th>\n",
       "      <th>ask_size</th>\n",
       "      <th>wap</th>\n",
       "      <th>target</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3180602.69</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380276.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>60651.50</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>8493.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.029704</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166603.91</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1642214.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>3233.04</td>\n",
       "      <td>1.000660</td>\n",
       "      <td>20605.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.519986</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>302879.87</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>1819368.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>37956.00</td>\n",
       "      <td>1.000298</td>\n",
       "      <td>18995.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.389950</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11917682.27</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>18389745.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>2324.90</td>\n",
       "      <td>1.000214</td>\n",
       "      <td>479032.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.010200</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>447549.96</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>17860614.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>16485.54</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>434.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.349849</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
       "0         0        0                  0      3180602.69   \n",
       "1         1        0                  0       166603.91   \n",
       "2         2        0                  0       302879.87   \n",
       "3         3        0                  0     11917682.27   \n",
       "4         4        0                  0       447549.96   \n",
       "\n",
       "   imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
       "0                        1         0.999812   13380276.64        NaN   \n",
       "1                       -1         0.999896    1642214.25        NaN   \n",
       "2                       -1         0.999561    1819368.03        NaN   \n",
       "3                       -1         1.000171   18389745.62        NaN   \n",
       "4                       -1         0.999532   17860614.95        NaN   \n",
       "\n",
       "   near_price  bid_price  bid_size  ask_price   ask_size  wap    target  \\\n",
       "0         NaN   0.999812  60651.50   1.000026    8493.03  1.0 -3.029704   \n",
       "1         NaN   0.999896   3233.04   1.000660   20605.09  1.0 -5.519986   \n",
       "2         NaN   0.999403  37956.00   1.000298   18995.00  1.0 -8.389950   \n",
       "3         NaN   0.999999   2324.90   1.000214  479032.40  1.0 -4.010200   \n",
       "4         NaN   0.999394  16485.54   1.000016     434.10  1.0 -7.349849   \n",
       "\n",
       "   time_id row_id  \n",
       "0        0  0_0_0  \n",
       "1        0  0_0_1  \n",
       "2        0  0_0_2  \n",
       "3        0  0_0_3  \n",
       "4        0  0_0_4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d1460-5d8b-4863-bcca-91ad334413cf",
   "metadata": {},
   "source": [
    "## Calculate rolling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f21529-59ab-4816-afec-2192d49a4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_features(input_df):\n",
    "    window_rsi=14\n",
    "    rsi_ma_window = 14\n",
    "    window_kd=14\n",
    "    smooth_d=3\n",
    "    min_lag = 1\n",
    "    short_MACD_window = 6\n",
    "    long_MACD_window = 13\n",
    "    signal_MACD_window = 5\n",
    "    \n",
    "    # for historical volatility, do it cross day\n",
    "    \n",
    "    # Group by date_id and calculate rolling values within each group\n",
    "    grouped = input_df.groupby('date_id')\n",
    "    \n",
    "    # Calculate price changes\n",
    "    input_df['delta'] = grouped['reference_price'].transform(lambda x: x.diff())\n",
    "    \n",
    "    # Separate gains (positive changes) and losses (negative changes)\n",
    "    input_df['gains'] = input_df['delta'].where(input_df['delta'] > 0, 0)\n",
    "    input_df['losses'] = -input_df['delta'].where(input_df['delta'] < 0, 0)\n",
    "\n",
    "    # Calculate average gains and average losses over the specified window\n",
    "    input_df['avg_gains'] = grouped['gains'].transform(lambda x: x.rolling(window=window_rsi, min_periods=1).mean())\n",
    "    input_df['avg_losses'] = grouped['losses'].transform(lambda x: x.rolling(window=window_rsi, min_periods=1).mean())\n",
    "\n",
    "    # Calculate relative strength (RS)\n",
    "    input_df['rs'] = input_df['avg_gains'] / input_df['avg_losses']\n",
    "    \n",
    "    # Calculate the RSI index\n",
    "    input_df['rsi'] = 100 - (100 / (1 + input_df['rs']))\n",
    "    \n",
    "    # Calculate the moving average of RSI\n",
    "    input_df['rsi_ma'] = grouped['rsi'].transform(lambda x: x.rolling(window=rsi_ma_window, min_periods=1).mean())\n",
    "\n",
    "    # Calculate rsi rsi_ma difference\n",
    "    input_df['rsi_rsi_ma_diff'] = input_df['rsi'] - input_df['rsi_ma']\n",
    "    \n",
    "    #################################################\n",
    "    # Calculate Stochastic Oscillator (%K and %D) for a given stock.\n",
    "    input_df['low_min'] = grouped['reference_price'].transform(lambda x: x.rolling(window=window_kd, min_periods=1).min())\n",
    "    input_df['high_max'] = grouped['reference_price'].transform(lambda x: x.rolling(window=window_kd, min_periods=1).max())\n",
    "\n",
    "    # Calculate %K\n",
    "    input_df['percentK'] = ((input_df['reference_price'] - input_df['low_min']) / \n",
    "                        (input_df['high_max'] - input_df['low_min']) * 100)\n",
    "    \n",
    "    # Smooth %K to get %D common choices of smooth_d are 3 or 5. Default is 3\n",
    "    input_df['percentD'] = grouped['percentK'].transform(lambda x: x.rolling(window=smooth_d, min_periods=1).max())\n",
    "\n",
    "    # Calculate KD difference\n",
    "    input_df['KD_diff'] = input_df['percentK'] - input_df['percentD']\n",
    "\n",
    "    ###############################################\n",
    "    # MACD\n",
    "    # default period set to short: 6, long: 13. Standard choices are 12 and 26, but may be too long... only 10 mins for each day, ~ 50 data points\n",
    "    # Calculate short-term EMA \n",
    "    input_df['short_ema'] = grouped['reference_price'].transform(lambda x: x.ewm(span=short_MACD_window, adjust=False, min_periods=1).mean())\n",
    "    # Calculate long-term EMA\n",
    "    input_df['long_ema'] = grouped['reference_price'].transform(lambda x: x.ewm(span=long_MACD_window, adjust=False, min_periods=1).mean())\n",
    "    \n",
    "    # Calculate MACD line\n",
    "    input_df['MACD'] = input_df['short_ema'] - input_df['long_ema']\n",
    "    \n",
    "    # Calculate signal line\n",
    "    input_df['Signal_Line'] = grouped['MACD'].transform(lambda x: x.ewm(span=signal_MACD_window, adjust=False, min_periods=1).mean())\n",
    "\n",
    "    # Calculate MACD_signal difference\n",
    "    input_df['MACD_Signal_Diff'] = input_df['MACD'] - input_df['Signal_Line']\n",
    "\n",
    "    \n",
    "    ###################################\n",
    "    # Bollings band\n",
    "    # feel it is not a good one to use... hate this indicator\n",
    "\n",
    "    ###################################\n",
    "    # rolling feature of Optimal order book spread\n",
    "    \n",
    "\n",
    "    # features to be dropped ['delta', 'gains', 'losses', 'avg_gains', 'avg_losses', 'rs', 'short_ema', 'long_ema', ]\n",
    "    \n",
    "    # lagged features (only do lagged feature for prices, for other rolling features, use derivatives)\n",
    "    prices = ['reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n",
    "    for a in prices:\n",
    "        input_df[f'{a}_lag_by_{min_lag}'] = grouped[a].transform(lambda x: x.shift(min_lag))\n",
    "        input_df[f'{a}_lag_by_{min_lag+1}'] = grouped[a].transform(lambda x: x.shift(min_lag+1))\n",
    "        input_df[f'{a}_lag_by_{min_lag+2}'] = grouped[a].transform(lambda x: x.shift(min_lag+2))\n",
    "\n",
    "    \n",
    "    otherLaggedFeatures = ['rsi', 'rsi_ma', 'rsi_rsi_ma_diff', 'percentK', 'percentD', 'KD_diff', 'MACD', \n",
    "                           'Signal_Line', 'MACD_Signal_Diff']\n",
    "    for a in prices:\n",
    "        input_df[f'{a}_1st_derivative'] = grouped[a].transform(lambda x: x.diff())\n",
    "        input_df[f'{a}_2nd_derivative'] = grouped[f'{a}_1st_derivative'].transform(lambda x: x.diff())\n",
    "                           \n",
    "    return input_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b3355-4a1d-413e-9365-0831f217b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_stock_date_pair(stock_id, input_df):\n",
    "    stock_data = input_df[input_df['stock_id'] == stock_id].copy()\n",
    "    calculate_rsi_stochastic_oscillator(stock_data, window_rsi=14, rsi_ma_window=14, window_kd=14, smooth_d=3)\n",
    "    return stock_id, stock_data[['rsi', 'rsi_ma', 'percentK', 'percentD']]\n",
    "\n",
    "def update_train_dataset(stock_id, result_data, input_df):\n",
    "    with lock:\n",
    "        mask = (input_df['stock_id'] == stock_id)\n",
    "        input_df.loc[mask, ['rsi', 'rsi_ma', 'percentK', 'percentD']] = result_data.values\n",
    "\n",
    "def parallelize_processing(unique_stock_ids, input_df):\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Create a list of tuples containing unique_stock_ids\n",
    "        combinations = [(stock_id, input_df) for stock_id in unique_stock_ids]\n",
    "\n",
    "        # Use executor.map to get results in the order they were submitted\n",
    "        results = list(tqdm(executor.map(lambda stock_id: process_stock_date_pair(stock_id, input_df), unique_stock_ids), total=len(unique_stock_ids)))\n",
    "\n",
    "        print(\"Processing completed. Starting result update.\")\n",
    "\n",
    "        # Update input_df with the results\n",
    "        list(tqdm(executor.map(lambda args: update_train_dataset(*args, input_df),\n",
    "                               [(stock_id, result_data) for stock_id, result_data in results]),\n",
    "                  total=len(results)))\n",
    "\n",
    "\n",
    "# Get unique stock and date IDs\n",
    "unique_stock_ids = df['stock_id'].unique()\n",
    "\n",
    "# Parallelize the processing\n",
    "parallelize_processing(unique_stock_ids, df)\n",
    "\n",
    "# drop useless columns\n",
    "useless_columns = ['gains', 'losses', 'avg_gains', 'avg_losses', 'rs', 'short_ema', 'long_ema', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f3265-da93-4e10-8e40-94606772ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simulate synthetic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3277cb-ebc5-4e34-aca5-b06e9b40cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd21dd2-da86-4671-9a4d-11f6fc4aa85e",
   "metadata": {},
   "source": [
    "## Memory reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7adca4b-941b-4045-9201-1539bdfdbec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(input_df, verbose=0):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    start_mem = input_df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in input_df.columns:\n",
    "        col_type = input_df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            # grab the minimum and maximum values in the column \n",
    "            c_min = input_df[col].min()\n",
    "            c_max = input_df[col].max()\n",
    "            # Depending on the range of values, \n",
    "            # it converts the column to the smallest integer data type that can accommodate the data while reducing memory usage. \n",
    "            # It checks for int8, int16, int32, and int64 data types based on the data range.\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    input_df[col] = input_df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    input_df[col] = input_df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    input_df[col] = input_df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    input_df[col] = input_df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    input_df[col] = input_df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    input_df[col] = input_df[col].astype(np.float32)\n",
    "                else:\n",
    "                    input_df[col] = input_df[col].astype(np.float32)\n",
    "    # If verbose is set to a truthy value (e.g., 1), it provides information about memory optimization, \n",
    "    # including the initial and final memory usage, and the percentage reduction in memory usage.\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = input_df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return input_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870acd3-bedf-494c-ba92-846dd9525668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
